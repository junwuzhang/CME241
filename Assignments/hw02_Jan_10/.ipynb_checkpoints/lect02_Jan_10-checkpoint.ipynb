{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write out the MP/MRP/MDP/Policy definitions and MRP/MDP Value Function definitions in your won style/notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answers to this question are as follows:\n",
    "- MP: Markov Process; a random process where the current state depend only on the most recent state\n",
    "- MRP: Markov Reward Process; a sequence of possible Markov states with values in each state and probabilities from one state to another\n",
    "- MDP: Markov Decision Process; a Markov Reward Process with decisions, in other words, the transition of states does not rely solely on probabilities, rather, they depend on decisions made when being in a state\n",
    "- Policy: a sequence of decisions made in a Markov Decision Process\n",
    "- MRP Value Function: The expected (sum of) return, starting from state $s$ in an MDP, and follow the policy $\\pi$\n",
    "- MDP Value Function: The expected (sum of) return, starting from state $s$ in an MDP, taking action $a$ at each state, and follow the policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Think about the data structures/class design to represent MP/MRP/MDP/Policy/Value Functions and implement them with clear type declarations\n",
    "    - Remember - your data structure/code design must resemble the Mathematical/notational formalism as much as possible\n",
    "    - Specifically the data structure/code design of MRP/MDP should be incremental (and not independent) to that of MP/MRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c9e4ed790b99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Value Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprevious_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'states' is not defined"
     ]
    }
   ],
   "source": [
    "# Markov Process\n",
    "class MP:\n",
    "    def __init__(self, states, transition_matrix):\n",
    "        self.states_ = states\n",
    "        self.transition_matrix_ = transition_matrix\n",
    "\n",
    "# Markov Reward Process\n",
    "class MRP:\n",
    "    def __init__(self, states_value_pair, transition_matrix, gamma):\n",
    "        self.states_value_pair_ = states_value_pair\n",
    "        self.transition_matrix_ = transition_matrix\n",
    "        self.gamma_ = gamma\n",
    "    def reward(state):\n",
    "        # return the value corresponding with the state\n",
    "        return states_value_pair[state][1]\n",
    "\n",
    "# Markov Decision Process\n",
    "class MDP:\n",
    "    def __init__(self, state_value_pair, transition_matrix, action_matrix, gamma):\n",
    "        self.states_value_pair_ = states_value_pair\n",
    "        # transition matrix \n",
    "        self.transition_matrix_ = transition_matrix\n",
    "        self.gamma_ = discount_factor\n",
    "        self.action_set = action_set\n",
    "    # return the next state based on the current state and action chosen being in that state\n",
    "    def nextState(current_state, action_idx):\n",
    "        action = action_set[action_idx]\n",
    "        # transition matrix should be a dictionary pairing up the next state with current state and action\n",
    "        return transition_matrix[(current_state, action)]\n",
    "    \n",
    "# Policy\n",
    "# policy should be a list of actions\n",
    "policy = []\n",
    "#example action is \"up\"\n",
    "action = \"up\"\n",
    "policy.append(action);\n",
    "        \n",
    "# Value Function\n",
    "# parameters\n",
    "for state in states:\n",
    "    for action in actions:\n",
    "        previous_value = value\n",
    "        value += (reward(state) + gamma * previous_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Separately implement the $r(s,s')$ and the $R(s) = \\sum_{s'} p(s,s') * r(s,s')$ definitions of MRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write code to convert/cast the $r(s,s')$ definition of MRP to the $R(s)$ definition of MRP (put some thought into code design here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write code to create a MRP given a MDP and a Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states:\n",
    "    # create action-value pair\n",
    "    next_state = transformation_matrix[(state, policy[action])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write out the MDP/MRP Bellman Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For MRP, the Bellman Equation is: $v^\\ast(s) =\\max_{a}R^a_s + \\gamma \\sum_{s'\\in S}P^a_{ss'}v^\\ast (s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write code to calculate MRP Value Function (based on Matrix inversion method you learnt in this lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write code to generate the stationary distribution for an MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
