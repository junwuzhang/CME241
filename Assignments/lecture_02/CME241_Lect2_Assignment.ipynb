{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write out the MP/MRP/MDP/Policy definitions and MRP/MDP Value Function definitions in your won style/notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MP: Markov Process; a random process where the current state depend only on the most recent state\n",
    "- MRP: Markov Reward Process; a sequence of possible Markov states with values in each state and probabilities from one state to another\n",
    "- MDP: Markov Decision Process; a Markov Reward Process with decisions, in other words, the transition of states does not rely solely on probabilities, rather, they depend on decisions made when being in a state\n",
    "- Policy: a sequence of decisions made in a Markov Decision Process\n",
    "- MRP Value Function: The expected (sum of) return, starting from state $s$ in an MDP, and follow the policy $\\pi$\n",
    "- MDP Value Function: The expected (sum of) return, starting from state $s$ in an MDP, taking action $a$ at each state, and follow the policy $\\pi$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
