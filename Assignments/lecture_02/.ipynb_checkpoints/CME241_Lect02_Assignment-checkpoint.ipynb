{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write out the MP/MRP/MDP/Policy definitions and MRP/MDP Value Function definitions in your won style/notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answers to this question are as follows:\n",
    "- MP: Markov Process; a random process where the current state depend only on the most recent state\n",
    "- MRP: Markov Reward Process; a sequence of possible Markov states with values in each state and probabilities from one state to another\n",
    "- MDP: Markov Decision Process; a Markov Reward Process with decisions, in other words, the transition of states does not rely solely on probabilities, rather, they depend on decisions made when being in a state\n",
    "- Policy: a sequence of decisions made in a Markov Decision Process\n",
    "- MRP Value Function: The expected (sum of) return, starting from state $s$ in an MDP, and follow the policy $\\pi$\n",
    "- MDP Value Function: The expected (sum of) return, starting from state $s$ in an MDP, taking action $a$ at each state, and follow the policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Think about the data structures/class design to represent MP/MRP/MDP/Policy/Value Functions and implement them with clear type declarations\n",
    "    - Remember - your data structure/code design must resemble the Mathematical/notational formalism as much as possible\n",
    "    - Specifically the data structure/code design of MRP/MDP should be incremental (and not independent) to that of MP/MRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov Process\n",
    "class MP:\n",
    "    def __init__(self, states, transition_matrix):\n",
    "        self.states_ = states\n",
    "        self.transition_matrix_ = transition_matrix\n",
    "\n",
    "# Markov Reward Process\n",
    "class MRP:\n",
    "    def __init__(self, states_value_pair, transition_matrix, gamma):\n",
    "        self.states_value_pair_ = states_value_pair\n",
    "        self.transition_matrix_ = transition_matrix\n",
    "        self.gamma_ = gamma\n",
    "    def reward(state):\n",
    "        # return the value corresponding with the state\n",
    "        return states_value_pair[state][1]\n",
    "\n",
    "# Markov Decision Process\n",
    "class MDP:\n",
    "    def __init__(self, state_value_pair, transition_matrix, action_matrix, gamma):\n",
    "        self.states_value_pair_ = states_value_pair\n",
    "        # transition matrix \n",
    "        self.transition_matrix_ = transition_matrix\n",
    "        self.gamma_ = discount_factor\n",
    "        self.action_set = action_set\n",
    "    # return the next state based on the current state and action chosen being in that state\n",
    "    def nextState(current_state, action_idx):\n",
    "        action = action_set[action_idx]\n",
    "        # transition matrix should be a dictionary pairing up the next state with current state and action\n",
    "        return transition_matrix[(current_state, action)]\n",
    "    \n",
    "# Policy\n",
    "# policy should be a list of actions\n",
    "policy = []\n",
    "#example action is \"up\"\n",
    "action = \"up\"\n",
    "policy.append(action);\n",
    "        \n",
    "# Value Function\n",
    "# parameters\n",
    "for state in states:\n",
    "    for action in actions:\n",
    "        previous_value = value\n",
    "        value += (reward(state) + gamma * previous_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Separately implement the $r(s,s')$ and the $R(s) = \\sum_{s'} p(s,s') * r(s,s')$ definitions of MRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write code to convert/cast the $r(s,s')$ definition of MRP to the $R(s)$ definition of MRP (put some thought into code design here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write code to create a MRP given a MDP and a Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write out the MDP/MRP Bellman Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For MRP, the Bellman Equation is: $v^\\ast(s) =\\max_{a}R^a_s + \\gamma \\sum_{s'\\in S}P^a_{ss'}v^\\ast (s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write code to calculate MRP Value Function (based on Matrix inversion method you learnt in this lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write code to generate the stationary distribution for an MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
