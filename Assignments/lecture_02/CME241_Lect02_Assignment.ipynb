{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write out the MP/MRP/MDP/Policy definitions and MRP/MDP Value Function definitions in your won style/notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answers to this question are as follows:\n",
    "- MP: Markov Process; a random process where the current state depend only on the most recent state\n",
    "- MRP: Markov Reward Process; a sequence of possible Markov states with values in each state and probabilities from one state to another\n",
    "- MDP: Markov Decision Process; a Markov Reward Process with decisions, in other words, the transition of states does not rely solely on probabilities, rather, they depend on decisions made when being in a state\n",
    "- Policy: a sequence of decisions made in a Markov Decision Process\n",
    "- MRP Value Function: The expected (sum of) return, starting from state $s$ in an MDP, and follow the policy $\\pi$\n",
    "- MDP Value Function: The expected (sum of) return, starting from state $s$ in an MDP, taking action $a$ at each state, and follow the policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Think about the data structures/class design to represent MP/MRP/MDP/Policy/Value Functions and implement them with clear type declarations\n",
    "    - Remember - your data structure/code design must resemble the Mathematical/notational formalism as much as possible\n",
    "    - Specifically the data structure/code design of MRP/MDP should be incremental (and not independent) to that of MP/MRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov Process\n",
    "class MP:\n",
    "    def __init__(self, states):\n",
    "        self.states = states\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Separately implement the $r(s,s')$ and the $R(s) = \\sum_{s'} p(s,s') * r(s,s')$ definitions of MRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write code to convert/cast the $r(s,s')$ definition of MRP to the $R(s)$ definition of MRP (put some thought into code design here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write code to create a MRP given a MDP and a Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write out the MDP/MRP Bellman Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For MRP, the Bellman Equation is: $v^\\ast(s) =\\max_{a}R^a_s + \\gamma \\sum_{s'\\in S}P^a_{ss'}v^\\ast (s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write code to calculate MRP Value Function (based on Matrix inversion method you learnt in this lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write code to generate the stationary distribution for an MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
